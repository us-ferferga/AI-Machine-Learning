{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca55ead",
   "metadata": {},
   "source": [
    "# Aprendizaje automático relacional\n",
    "\n",
    "#### Fernando Jesús Fernández Gallardo\n",
    "#### Carmen Galván López"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979dd0c",
   "metadata": {},
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda48683",
   "metadata": {},
   "source": [
    "#### Imports y variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing, model_selection, naive_bayes\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "semilla = 86\n",
    "test_size= .33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d4c7b",
   "metadata": {},
   "source": [
    "#### Lectura y procesamiento inicial de los datos brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leemos los archivos\n",
    "aristas = read_csv('data/political-books-edges.csv')\n",
    "vertices = read_csv('data/political-books-nodes.csv')\n",
    "\n",
    "#Borramos la columna ID\n",
    "del(vertices['Id'])\n",
    "\n",
    "#Mostramos las primeras 35 filas\n",
    "vertices.head(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366b383",
   "metadata": {},
   "source": [
    "#### Selección y validación de los datos brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprobamos que el dataset es válido verificando que no existen duplicados\n",
    "\"\"\"\n",
    "if len(vertices) != len(set(vertices['Label'])):\n",
    "    raise ValueError(\"El dataset no es válido ya que contiene duplicados\")\n",
    "\"\"\"\n",
    "La mejor forma de identificar cada uno de los elementos que forma parte\n",
    "del conjunto de entrenamiento es el nombre del propio libro (que en el dataset\n",
    "se llama 'Label') en vez del ID o cualquier otro tipo de indentificador más\n",
    "complejo. De esta forma, también es más fácil identificar elementos duplicados\n",
    "(si los hubiera)\n",
    "\"\"\"\n",
    "atributos = vertices['Label']\n",
    "\"\"\"\n",
    "Nuestro objetivo es predecir la ideología política del autor basándonos en\n",
    "sus obras, por lo que el objetivo que perseguimos en nuestro modelo\n",
    "es el de la ideología política\n",
    "\"\"\"\n",
    "objetivo = vertices['political_ideology']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904808a",
   "metadata": {},
   "source": [
    "## Inicio del entrenamiento\n",
    "#### Codificación del objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Para poder trabajar con los datos que tenemos, necesitamos convertirlos en un formato que sklearn pueda \"entender\".\n",
    "Debemos de hacer que nuestros datos \"planos\" sean para sklearn objetos \"comparables\", dependiendo del tipo de\n",
    "ordenación que nosotros veamos más apropiada para el método en cuestión\n",
    "(de una manera similar hacemos en Java cuando implementamos la interfaz 'Comparable' y el método compareTo)\n",
    "\n",
    "El codificador adecuado para la variable objetivo es LabelEncoder, que trabaja\n",
    "con una lista o array unidimensional de sus valores y admite cadenas\n",
    "\n",
    "\"\"\"\n",
    "# Codificadores\n",
    "codificador_atributos = preprocessing.LabelEncoder()\n",
    "codificador_objetivo = preprocessing.LabelEncoder()\n",
    "# Datos codificados\n",
    "atributos_codificados = codificador_atributos.fit_transform(atributos)\n",
    "objetivo_codificado = codificador_objetivo.fit_transform(objetivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b9190",
   "metadata": {},
   "source": [
    "#### División en conjunto de entrenamiento y conjunto de prueba\n",
    "\n",
    "Partimos el atributo y el objetivo en dos, de entrenamiento y de prueba. Esto se hace para evitar el sobreajuste, que es cuando el modelo se adapta demasiado a los datos de entrenamiento y pierde capacidad de generalizar a nuevos casos.\n",
    "\n",
    "Reservamos con `test_size` el 33% del total de los datos para la posterior prueba. El resto se usa para el entrenamiento.\n",
    "\n",
    "Con `stratify` indicamos la división que debe mantener la proporción de clases en ambos conjuntos, es decir, que si hay un 40% de conservadores, un 30% de neutrales y un 30% de liberales en el conjunto original, también haya esa proporción en el conjunto de entrenamiento y en el de prueba. Esto se hace para evitar sesgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "(atributos_entrenamiento,\n",
    " atributos_prueba,\n",
    " objetivo_entrenamiento,\n",
    " objetivo_prueba) = model_selection.train_test_split(\n",
    "        atributos_codificados,\n",
    "        objetivo_codificado,\n",
    "        # Valor de la semilla aleatoria para que el muestreo sea reproducible a pesar de ser aleatorio\n",
    "        random_state=semilla,\n",
    "        test_size=test_size,\n",
    "        stratify=objetivo_codificado\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b90d68",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "#### No relacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "El método reshape solo cambia la forma del array, pero no su contenido.\n",
    "El clasificador MultinomialNB de sklearn espera un array 2D porque puede manejar\n",
    "múltiples características por muestra, pero en nuestro caso solo tenemos una característica por muestra,\n",
    "que es el nombre del libro.\n",
    "\"\"\"\n",
    "atr_cod_reshaped = atributos_codificados.reshape(-1, 1)\n",
    "atr_pr_reshaped = atributos_prueba.reshape(-1, 1)\n",
    "atr_ent_reshaped = atributos_entrenamiento.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76cac28",
   "metadata": {},
   "source": [
    "## Explicación de Bing, borrar antes de entregar:\n",
    "\n",
    "```\n",
    "Lo que haces con ese código es crear y entrenar un clasificador de Naive Bayes no relacional, que es un tipo de modelo que asume que los atributos son independientes entre sí dada la clase. Es decir, que la probabilidad de que un libro tenga un cierto título no depende de la probabilidad de que tenga otra característica, como el autor o el género. Esto simplifica el cálculo de las probabilidades, pero puede ser una suposición muy fuerte en algunos casos.\n",
    "\n",
    "El clasificador que usas es el MultinomialNB, que es adecuado para atributos discretos que representan frecuencias o conteos. Por ejemplo, si los títulos de los libros se representan como vectores de palabras, donde cada posición indica cuántas veces aparece una palabra en el título. El parámetro alpha es el suavizado, que es una técnica para evitar que las probabilidades sean cero cuando hay palabras que no aparecen en el conjunto de entrenamiento. Un valor típico es 1, pero puedes probar otros valores para ver cómo afectan al rendimiento del modelo.\n",
    "\n",
    "El método fit es el que entrena el modelo con los datos de entrenamiento. Le pasas los atributos y los objetivos codificados, y el modelo aprende las probabilidades de cada clase y de cada atributo dado cada clase. Estas probabilidades se almacenan en los atributos class_log_prior_ y feature_log_prob_ del modelo.\n",
    "\n",
    "El bucle for que haces al final es para mostrar la cantidad de ejemplos y el logaritmo de la probabilidad de cada clase. El logaritmo se usa para evitar problemas de desbordamiento o subdesbordamiento cuando se multiplican muchas probabilidades pequeñas. La clase 0 corresponde a conservador, la 1 a neutral y la 2 a liberal.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "suavizado = 1\n",
    "\n",
    "clasif_NB = naive_bayes.MultinomialNB(alpha=suavizado)\n",
    "clasif_NB.fit(atr_ent_reshaped, objetivo_entrenamiento)\n",
    "\n",
    "#Calculamos la cantidad de ejemplos para cada clase y los logaritmos\n",
    "for clase, cantidad_ejemplos_clase, log_probabilidad_clase in zip(\n",
    "        clasif_NB.classes_, clasif_NB.class_count_, clasif_NB.class_log_prior_):\n",
    "    print(f\"Cantidad de ejemplos para la clase {0}: {1}\", clase, cantidad_ejemplos_clase)\n",
    "    print(f\"Logaritmo de la probabilidad aprendida para la clase {0}: {1}\", clase, log_probabilidad_clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eefd783",
   "metadata": {},
   "source": [
    "## EXPLICACIÓN DE BING, BORRAR\n",
    "\n",
    "```\n",
    "Lo que haces con ese código es evaluar el rendimiento del modelo de Naive Bayes que has entrenado. Para ello, usas varias técnicas:\n",
    "\n",
    "- Primero, usas el método predict para obtener las predicciones del modelo para los datos de prueba. Le pasas los atributos de prueba codificados y te devuelve un arreglo con los objetivos predichos. Estos los puedes comparar con los objetivos reales para ver cuántos aciertos y errores tiene el modelo.\n",
    "- Luego, usas el método score para obtener la precisión del modelo, que es la proporción de aciertos sobre el total de casos. Le pasas los atributos y los objetivos de prueba codificados y te devuelve un valor entre 0 y 1, donde 1 significa que el modelo acierta todos los casos y 0 que falla todos.\n",
    "- Después, usas la técnica de cross validation, que es una forma de estimar la precisión del modelo usando diferentes particiones de los datos. Creas un objeto ShuffleSplit que define cómo se van a dividir los datos en cada iteración. En este caso, haces 10 iteraciones, reservando el 33% de los datos para la prueba y usando la misma semilla aleatoria que antes. Luego, usas la función cross_val_score para obtener la precisión del modelo en cada iteración. Le pasas el modelo, los atributos y los objetivos codificados de todo el conjunto de datos y el objeto ShuffleSplit. Te devuelve un arreglo con las precisiones obtenidas en cada iteración.\n",
    "- Finalmente, usas la función np.mean para obtener la media de las precisiones obtenidas con cross validation. Esto te da una idea de cómo se comporta el modelo en promedio con diferentes conjuntos de datos.\n",
    "\n",
    "Con todo esto, puedes tener una medida más robusta y confiable del rendimiento del modelo de Naive Bayes. Espero haberte ayudado a entender mejor lo que está sucediendo en tu código. 😊\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ShuffleSplit es necesario para la CrossValidation\n",
    "\"\"\"\n",
    "cv = ShuffleSplit(n_splits=10, test_size=test_size, random_state=semilla)\n",
    "\n",
    "\n",
    "#Probamos la predicción con los atributos de prueba\n",
    "print('Predicción con Naive Bayes: ', clasif_NB.predict(atr_pr_reshaped))\n",
    "#Hacemos el score con naive bayes\n",
    "print('Precisión con Naive Bayes: ', clasif_NB.score(atr_pr_reshaped, objetivo_prueba))\n",
    "#Hacemos el score con cross validation\n",
    "print('Precisión con cross validation: ', cross_val_score(clasif_NB, atr_cod_reshaped, objetivo_codificado, cv=cv))\n",
    "#Hacemos la media de score de cross validation, ya que al final es lo que nos interesa\n",
    "print(f'Media de precisión: {0}', np.mean(cross_val_score(clasif_NB, atr_cod_reshaped, objetivo_codificado, cv=cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577a612a",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7dfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a utilizar este modelo ya que nos viene bien al tener un dataset pequeño. Este modelo aumenta\n",
    "#el tiempo de entrenamiento cuadrátricamente con el número de ejemplos\n",
    "\n",
    "classif_SVC = SVC().fit(atr_ent_reshaped, objetivo_entrenamiento)\n",
    "\n",
    "\n",
    "#Probamos la predicción con los atributos de prueba\n",
    "print('Predicción SVC: ', classif_SVC.predict(atr_pr_reshaped))\n",
    "#Hacemos el score con kNN\n",
    "print('Precisión SVC: ', classif_SVC.score(atr_pr_reshaped, objetivo_prueba))\n",
    "#Hacemos el score con cross validation\n",
    "print(f'Precisión cross validation: {0}', np.mean(cross_val_score(classif_SVC, atr_cod_reshaped, objetivo_codificado, cv=cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c740039",
   "metadata": {},
   "source": [
    "## Sacar métricas relacionales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
